{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d930a028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "# Auto select device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Load metadata\n",
    "# ---------------------------------------------------\n",
    "with open(\"/kaggle/input/cub2002011/CUB_200_2011/train_test_split.txt\") as f:\n",
    "    split = dict(line.strip().split() for line in f)\n",
    "\n",
    "with open(\"/kaggle/input/cub2002011/CUB_200_2011/images.txt\") as f:\n",
    "    paths = dict(line.strip().split() for line in f)\n",
    "\n",
    "with open(\"/kaggle/input/cub2002011/CUB_200_2011/image_class_labels.txt\") as f:\n",
    "    labels = dict(line.strip().split() for line in f)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Select only FIRST 5 classes\n",
    "# ---------------------------------------------------\n",
    "selected_classes = set(list({int(v) for v in labels.values()})[:200])\n",
    "\n",
    "print(\"Using classes:\", selected_classes)\n",
    "\n",
    "train_paths, train_labels = [], []\n",
    "test_paths, test_labels = [], []\n",
    "\n",
    "base = \"/kaggle/input/cub2002011/CUB_200_2011/images/\"\n",
    "\n",
    "for img_id, rel in paths.items():\n",
    "    cls = int(labels[img_id])\n",
    "    if cls not in selected_classes:\n",
    "        continue\n",
    "\n",
    "    full = base + rel\n",
    "    if split[img_id] == \"1\":\n",
    "        train_paths.append(full)\n",
    "        train_labels.append(cls)\n",
    "    else:\n",
    "        test_paths.append(full)\n",
    "        test_labels.append(cls)\n",
    "\n",
    "print(\"Train images:\", len(train_paths))\n",
    "print(\"Test images :\", len(test_paths))\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Convert to DataFrames (path + class)\n",
    "# ---------------------------------------------------\n",
    "train_df = pd.DataFrame({\"path\": train_paths, \"class\": train_labels})\n",
    "test_df  = pd.DataFrame({\"path\": test_paths , \"class\": test_labels})\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Per-class sample counts\n",
    "# ---------------------------------------------------\n",
    "\n",
    "train_count = train_df[\"class\"].value_counts().sort_index()\n",
    "test_count  = test_df[\"class\"].value_counts().sort_index()\n",
    "\n",
    "print(\"\\n===== TRAIN PER-CLASS COUNTS =====\")\n",
    "print(train_count)\n",
    "\n",
    "print(\"\\n===== TEST PER-CLASS COUNTS =====\")\n",
    "print(test_count)\n",
    "\n",
    "print(\"\\n===== SUMMARY =====\")\n",
    "print(\"Train: classes =\", train_count.index.nunique(),\n",
    "      \"| min =\", train_count.min(),\n",
    "      \"| max =\", train_count.max(),\n",
    "      \"| avg =\", train_count.mean())\n",
    "\n",
    "print(\"Test : classes =\", test_count.index.nunique(),\n",
    "      \"| min =\", test_count.min(),\n",
    "      \"| max =\", test_count.max(),\n",
    "      \"| avg =\", test_count.mean())\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Dataset class with transforms\n",
    "# ---------------------------------------------------\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "class CUBDataset(Dataset):\n",
    "    def __init__(self, df, transform):\n",
    "        self.paths = df[\"path\"].values\n",
    "        self.labels = df[\"class\"].values\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
    "        img = self.transform(img)\n",
    "        cls = self.labels[idx] - 1\n",
    "        return img, cls\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Dataloaders\n",
    "# ---------------------------------------------------\n",
    "train_dataset = CUBDataset(train_df, transform_train)\n",
    "test_dataset = CUBDataset(test_df, transform_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "test_loader  = DataLoader(test_dataset , batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "print(\"Train loader batches:\", len(train_loader))\n",
    "print(\"Test loader batches :\", len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3c20d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# 1. Imports & Device\n",
    "# --------------------------------------------------------------\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import normalized_mutual_info_score, f1_score, pairwise_distances\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 2. Seed\n",
    "# --------------------------------------------------------------\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_seed(42)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 3. Load CUB metadata\n",
    "# --------------------------------------------------------------\n",
    "base_dir = \"/kaggle/input/cub2002011/CUB_200_2011\"\n",
    "\n",
    "with open(os.path.join(base_dir, \"train_test_split.txt\")) as f:\n",
    "    split_dict = {k: int(v) for k, v in (l.strip().split() for l in f)}\n",
    "with open(os.path.join(base_dir, \"images.txt\")) as f:\n",
    "    path_dict = dict(l.strip().split() for l in f)\n",
    "with open(os.path.join(base_dir, \"image_class_labels.txt\")) as f:\n",
    "    label_dict = dict(l.strip().split() for l in f)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 4. Zero-shot split: first 100 train, last 100 test\n",
    "# --------------------------------------------------------------\n",
    "all_classes = sorted({int(v) for v in label_dict.values()})\n",
    "train_classes = set(all_classes[:100])\n",
    "test_classes  = set(all_classes[100:200])\n",
    "\n",
    "train_paths, train_labels = [], []\n",
    "test_paths , test_labels  = [], []\n",
    "\n",
    "img_base = os.path.join(base_dir, \"images\")\n",
    "for img_id, rel in path_dict.items():\n",
    "    cls = int(label_dict[img_id])\n",
    "    if cls not in train_classes and cls not in test_classes: continue\n",
    "    full = os.path.join(img_base, rel)\n",
    "    is_train = split_dict[img_id] == 1\n",
    "\n",
    "    if cls in train_classes and is_train:\n",
    "        train_paths.append(full); train_labels.append(cls-1)\n",
    "    elif cls in test_classes and not is_train:\n",
    "        test_paths.append(full); test_labels.append(cls-1)\n",
    "\n",
    "train_df = pd.DataFrame({\"path\": train_paths, \"class\": train_labels})\n",
    "test_df  = pd.DataFrame({\"path\": test_paths , \"class\": test_labels})\n",
    "\n",
    "print(f\"Train (known): {len(train_df)} imgs, {len(train_classes)} classes\")\n",
    "print(f\"Test  (unseen): {len(test_df)} imgs, {len(test_classes)} classes\")\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 5. Transforms\n",
    "# --------------------------------------------------------------\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std =[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std =[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "class CUBDataset(Dataset):\n",
    "    def __init__(self, df, transform):\n",
    "        self.paths = df[\"path\"].values\n",
    "        self.labels = df[\"class\"].values\n",
    "        self.transform = transform\n",
    "    def __len__(self): return len(self.paths)\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
    "        img = self.transform(img)\n",
    "        return img, self.labels[idx]\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(CUBDataset(train_df, train_transform),\n",
    "                          batch_size=batch_size, shuffle=True,\n",
    "                          num_workers=4, pin_memory=True, drop_last=True)\n",
    "test_loader  = DataLoader(CUBDataset(test_df, test_transform),\n",
    "                          batch_size=batch_size, shuffle=False,\n",
    "                          num_workers=4, pin_memory=True)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 6. Simple AutoEncoder Model\n",
    "# --------------------------------------------------------------\n",
    "class SimpleMetricAutoEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim=128):\n",
    "        super().__init__()\n",
    "        goog = models.googlenet(weights=\"IMAGENET1K_V1\")\n",
    "        self.backbone = nn.Sequential(\n",
    "            goog.conv1, goog.maxpool1,\n",
    "            goog.conv2, goog.conv3, goog.maxpool2,\n",
    "            goog.inception3a, goog.inception3b, goog.maxpool3,\n",
    "            goog.inception4a, goog.inception4b, goog.inception4c,\n",
    "            goog.inception4d, goog.inception4e,\n",
    "            goog.maxpool4,\n",
    "            goog.inception5a, goog.inception5b,\n",
    "            goog.avgpool\n",
    "        )\n",
    "        self.fc_embed = nn.Linear(1024, embed_dim)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1024)\n",
    "        )\n",
    "\n",
    "    def extract_f(self, x):\n",
    "        f = self.backbone(x)\n",
    "        return f.view(f.size(0), -1)          # (B, 1024)\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = self.extract_f(x)\n",
    "        z = self.fc_embed(f)\n",
    "        f_hat = self.decoder(z)\n",
    "        return z, f, f_hat\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 7. Proxy-NCA++ Loss (Multiple Positive Proxies)\n",
    "# --------------------------------------------------------------\n",
    "class ProxyNCAPlusPlus(nn.Module):\n",
    "    def __init__(self, embed_dim, num_classes, num_pos=3, temp=0.1):\n",
    "        super().__init__()\n",
    "        self.proxies = nn.Parameter(torch.randn(num_classes, num_pos, embed_dim))\n",
    "        nn.init.xavier_uniform_(self.proxies)\n",
    "        self.temp = temp\n",
    "        self.num_pos = num_pos\n",
    "\n",
    "    def forward(self, embedding, label):\n",
    "        embedding = F.normalize(embedding, dim=1)          # (B, D)\n",
    "        proxies   = F.normalize(self.proxies, dim=-1)     # (C, K, D)\n",
    "\n",
    "        B, D = embedding.shape\n",
    "        C, K = proxies.shape[:2]\n",
    "\n",
    "        label_exp = label.unsqueeze(1).expand(B, K)       # (B, K)\n",
    "        pos_proxies = proxies[label_exp, torch.arange(K).to(device)]\n",
    "\n",
    "        sim_pos = torch.bmm(pos_proxies, embedding.unsqueeze(-1)).squeeze(-1) / self.temp\n",
    "        sim_pos = sim_pos.max(dim=1)[0]                   # (B,)\n",
    "\n",
    "        all_proxies = proxies.view(-1, D)                 # (C*K, D)\n",
    "        sim_all = F.linear(embedding, all_proxies) / self.temp\n",
    "        logsumexp = torch.logsumexp(sim_all, dim=1)\n",
    "\n",
    "        loss = -(sim_pos - logsumexp).mean()\n",
    "        return loss\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 8. Loss Helper\n",
    "# --------------------------------------------------------------\n",
    "def recon_loss(f, f_hat):\n",
    "    return F.mse_loss(f_hat, f)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 9. EVALUATION METRICS (MUST BE DEFINED BEFORE TRAINING)\n",
    "# --------------------------------------------------------------\n",
    "def recall_at_k(E, L, k=1):\n",
    "    sim = E @ E.T\n",
    "    np.fill_diagonal(sim, -np.inf)\n",
    "    idx = np.argpartition(-sim, k, axis=1)[:, :k]\n",
    "    correct = sum(L[i] in L[idx[i]] for i in range(len(L)))\n",
    "    return correct / len(L)\n",
    "\n",
    "def clustering_metrics(E, L, n_clusters):\n",
    "    km = KMeans(n_clusters=n_clusters, n_init=10, random_state=0)\n",
    "    pred = km.fit_predict(E)\n",
    "    nmi = normalized_mutual_info_score(L, pred)\n",
    "    f1  = f1_score(L, pred, average='macro')\n",
    "    return nmi, f1\n",
    "\n",
    "def pairwise_prec_recall(E, L):\n",
    "    D = pairwise_distances(E)\n",
    "    y_true = (L[:, None] == L[None, :]).astype(int)\n",
    "    thr = np.median(D)\n",
    "    y_pred = (D <= thr).astype(int)\n",
    "    tp = (y_pred * y_true).sum()\n",
    "    fp = (y_pred * (1 - y_true)).sum()\n",
    "    fn = ((1 - y_pred) * y_true).sum()\n",
    "    prec = tp / (tp + fp + 1e-12)\n",
    "    rec  = tp / (tp + fn + 1e-12)\n",
    "    return prec, rec\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 10. Model & Optimizer\n",
    "# --------------------------------------------------------------\n",
    "embed_dim   = 128\n",
    "num_classes = 100                     # only known classes\n",
    "\n",
    "model     = SimpleMetricAutoEncoder(embed_dim=embed_dim).to(device)\n",
    "proxy_nca = ProxyNCAPlusPlus(embed_dim, num_classes, num_pos=3, temp=0.1).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(model.parameters()) + list(proxy_nca.parameters()),\n",
    "    lr=1e-4,\n",
    "    weight_decay=1e-5\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 11. Validation split (for early-stop)\n",
    "# --------------------------------------------------------------\n",
    "val_ratio = 0.1\n",
    "val_df_list = []\n",
    "train_df_list = []\n",
    "for c in range(100):\n",
    "    sub = train_df[train_df[\"class\"] == c]\n",
    "    n_val = max(1, int(len(sub) * val_ratio))\n",
    "    idx = np.random.choice(sub.index, n_val, replace=False)\n",
    "    val_df_list.append(sub.loc[idx])\n",
    "    train_df_list.append(sub.drop(idx))\n",
    "\n",
    "val_df       = pd.concat(val_df_list).reset_index(drop=True)\n",
    "train_df_fin = pd.concat(train_df_list).reset_index(drop=True)\n",
    "\n",
    "train_loader = DataLoader(CUBDataset(train_df_fin, train_transform),\n",
    "                          batch_size=batch_size, shuffle=True,\n",
    "                          num_workers=4, pin_memory=True, drop_last=True)\n",
    "val_loader   = DataLoader(CUBDataset(val_df, test_transform),\n",
    "                          batch_size=batch_size, shuffle=False,\n",
    "                          num_workers=4, pin_memory=True)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 12. Validation helper (uses recall_at_k defined above)\n",
    "# --------------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def validate():\n",
    "    model.eval()\n",
    "    E, L = [], []\n",
    "    for x, y in val_loader:\n",
    "        x = x.to(device)\n",
    "        z, _, _ = model(x)\n",
    "        E.append(z.cpu())\n",
    "        L.append(y)\n",
    "    E = torch.cat(E)\n",
    "    L = torch.cat(L)\n",
    "    E = F.normalize(E, dim=1).numpy()\n",
    "    L = L.numpy()\n",
    "    return recall_at_k(E, L, k=1)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 13. Training Loop\n",
    "# --------------------------------------------------------------\n",
    "EPOCHS   = 100\n",
    "位_recon  = 1.0\n",
    "位_nca    = 1.0\n",
    "\n",
    "best_val_r1 = 0.0\n",
    "patience    = 15\n",
    "wait        = 0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    proxy_nca.train()\n",
    "    epoch_loss = 0.0\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        z, f, f_hat = model(imgs)\n",
    "        L_rec = recon_loss(f, f_hat)\n",
    "        L_nca = proxy_nca(z, labels)\n",
    "\n",
    "        loss = 位_recon * L_rec + 位_nca * L_nca\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # ------------------- validation -------------------\n",
    "    val_r1 = validate()\n",
    "    print(f\"Epoch {epoch:02d} | Loss: {epoch_loss/len(train_loader):.4f} | Val R@1: {val_r1:.4f}\")\n",
    "\n",
    "    if val_r1 > best_val_r1:\n",
    "        best_val_r1 = val_r1\n",
    "        torch.save(model.state_dict(), \"ae_proxy_best.pth\")\n",
    "        wait = 0\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "\n",
    "# Load best checkpoint\n",
    "model.load_state_dict(torch.load(\"ae_proxy_best.pth\"))\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 14. Feature extraction helper\n",
    "# --------------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def extract_feats(loader):\n",
    "    model.eval()\n",
    "    embs, lbls = [], []\n",
    "    for x, y in loader:\n",
    "        x = x.to(device)\n",
    "        z, _, _ = model(x)\n",
    "        embs.append(z.cpu())\n",
    "        lbls.append(y)\n",
    "    E = torch.cat(embs)\n",
    "    L = torch.cat(lbls)\n",
    "    E = F.normalize(E, dim=1).numpy()\n",
    "    L = L.numpy()\n",
    "    return E, L\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 15. TRAIN SET evaluation\n",
    "# --------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION ON TRAIN SET (100 known classes)\")\n",
    "print(\"=\"*60)\n",
    "train_E, train_L = extract_feats(train_loader)\n",
    "for k in [1,2,4,8]:\n",
    "    print(f\"Recall@{k}: {recall_at_k(train_E, train_L, k):.4f}\")\n",
    "nmi_tr, f1_tr = clustering_metrics(train_E, train_L, n_clusters=100)\n",
    "print(f\"NMI : {nmi_tr:.4f}\")\n",
    "print(f\"F1  : {f1_tr:.4f}\")\n",
    "p_tr, r_tr = pairwise_prec_recall(train_E, train_L)\n",
    "print(f\"Prec: {p_tr:.4f}  Rec: {r_tr:.4f}\")\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 16. TEST SET evaluation\n",
    "# --------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION ON TEST SET (100 unseen classes)\")\n",
    "print(\"=\"*60)\n",
    "test_E, test_L = extract_feats(test_loader)\n",
    "for k in [1,2,4,8]:\n",
    "    print(f\"Recall@{k}: {recall_at_k(test_E, test_L, k):.4f}\")\n",
    "nmi_te, f1_te = clustering_metrics(test_E, test_L, n_clusters=100)\n",
    "print(f\"NMI : {nmi_te:.4f}\")\n",
    "print(f\"F1  : {f1_te:.4f}\")\n",
    "p_te, r_te = pairwise_prec_recall(test_E, test_L)\n",
    "print(f\"Prec: {p_te:.4f}  Rec: {r_te:.4f}\")\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 17. PCA visualisation (test)\n",
    "# --------------------------------------------------------------\n",
    "def plot_pca(E, L, title=\"PCA\", n_samples=3000):\n",
    "    if len(E) > n_samples:\n",
    "        idx = np.random.choice(len(E), n_samples, replace=False)\n",
    "        E, L = E[idx], L[idx]\n",
    "    X = PCA(n_components=2).fit_transform(E)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    s = plt.scatter(X[:,0], X[:,1], c=L, cmap='tab20', s=15, alpha=0.7)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\n",
    "    plt.colorbar(s)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_pca(test_E, test_L, \"PCA of AutoEncoder + Proxy-NCA++ (100 Unseen Classes)\")\n",
    "\n",
    "\n",
    "\n",
    "def plot_tsne(E, L, title=\"t-SNE\", n_samples=3000, perplexity=30, random_state=42):\n",
    "    \"\"\"\n",
    "    Visualise embeddings E with labels L using t-SNE.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    E : np.ndarray, shape (N, D)\n",
    "        Embedding matrix.\n",
    "    L : np.ndarray, shape (N,)\n",
    "        Integer class labels.\n",
    "    title : str\n",
    "        Plot title.\n",
    "    n_samples : int\n",
    "        If N > n_samples, randomly subsample.\n",
    "    perplexity : float\n",
    "        t-SNE perplexity (typical values 5-50).\n",
    "    random_state : int\n",
    "        For reproducibility.\n",
    "    \"\"\"\n",
    "    if len(E) > n_samples:\n",
    "        idx = np.random.choice(len(E), n_samples, replace=False)\n",
    "        E, L = E[idx], L[idx]\n",
    "\n",
    "    # t-SNE (Barnes-Hut, O(N log N))\n",
    "    tsne = TSNE(\n",
    "        n_components=2,\n",
    "        perplexity=perplexity,\n",
    "        n_iter=1000,\n",
    "        random_state=random_state,\n",
    "        method='barnes_hut',   # fast approximate version\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    X_2d = tsne.fit_transform(E)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(\n",
    "        X_2d[:, 0], X_2d[:, 1],\n",
    "        c=L, cmap='tab20', s=15, alpha=0.7\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"t-SNE 1\")\n",
    "    plt.ylabel(\"t-SNE 2\")\n",
    "    plt.colorbar(scatter, label='Class')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Example usage (exactly like your PCA call)\n",
    "# ------------------------------------------------------------------\n",
    "plot_tsne(\n",
    "    test_E, test_L,\n",
    "    title=\"t-SNE of AutoEncoder + Proxy-NCA++ (100 Unseen Classes)\",\n",
    "    n_samples=3000,\n",
    "    perplexity=40          # tweak 20-50 for 100 classes\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
