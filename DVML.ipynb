{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d0ee45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# 1. Imports & device\n",
    "# --------------------------------------------------------------\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import normalized_mutual_info_score, f1_score, pairwise_distances\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 2. Seed everything\n",
    "# --------------------------------------------------------------\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 3. Load CUB metadata\n",
    "# --------------------------------------------------------------\n",
    "base_dir = \"/kaggle/input/cub2002011/CUB_200_2011\"\n",
    "\n",
    "with open(os.path.join(base_dir, \"train_test_split.txt\")) as f:\n",
    "    split_dict = {k: int(v) for k, v in (l.strip().split() for l in f)}\n",
    "with open(os.path.join(base_dir, \"images.txt\")) as f:\n",
    "    path_dict = dict(l.strip().split() for l in f)\n",
    "with open(os.path.join(base_dir, \"image_class_labels.txt\")) as f:\n",
    "    label_dict = dict(l.strip().split() for l in f)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 4. Zero-shot split (first 100 = train, last 100 = test)\n",
    "# --------------------------------------------------------------\n",
    "all_classes = sorted({int(v) for v in label_dict.values()})\n",
    "train_classes = set(all_classes[:100])   # known\n",
    "test_classes  = set(all_classes[100:200])# unseen\n",
    "\n",
    "train_paths, train_labels = [], []\n",
    "test_paths , test_labels  = [], []\n",
    "\n",
    "img_base = os.path.join(base_dir, \"images\")\n",
    "for img_id, rel in path_dict.items():\n",
    "    cls = int(label_dict[img_id])\n",
    "    if cls not in train_classes and cls not in test_classes:\n",
    "        continue\n",
    "    full = os.path.join(img_base, rel)\n",
    "    is_train = split_dict[img_id] == 1\n",
    "\n",
    "    if cls in train_classes and is_train:\n",
    "        train_paths.append(full); train_labels.append(cls-1)\n",
    "    elif cls in test_classes and not is_train:\n",
    "        test_paths.append(full); test_labels.append(cls-1)\n",
    "\n",
    "train_df = pd.DataFrame({\"path\": train_paths, \"class\": train_labels})\n",
    "test_df  = pd.DataFrame({\"path\": test_paths , \"class\": test_labels})\n",
    "\n",
    "print(f\"Train (known): {len(train_df)} imgs, {len(train_classes)} classes\")\n",
    "print(f\"Test  (unseen): {len(test_df)} imgs, {len(test_classes)} classes\")\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 5. Validation split from *known* classes (for early-stop)\n",
    "# --------------------------------------------------------------\n",
    "val_ratio = 0.15\n",
    "val_df_list = []\n",
    "train_df_list = []\n",
    "\n",
    "for c in train_classes:\n",
    "    sub = train_df[train_df[\"class\"] == c-1]\n",
    "    n_val = max(1, int(len(sub) * val_ratio))\n",
    "    val_idx = np.random.choice(sub.index, n_val, replace=False)\n",
    "    val_df_list.append(sub.loc[val_idx])\n",
    "    train_df_list.append(sub.drop(val_idx))\n",
    "\n",
    "val_df  = pd.concat(val_df_list).reset_index(drop=True)\n",
    "train_df = pd.concat(train_df_list).reset_index(drop=True)\n",
    "\n",
    "print(f\"Train (after val split): {len(train_df)}\")\n",
    "print(f\"Val   (known): {len(val_df)}\")\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 6. Transforms (paper exact)\n",
    "# --------------------------------------------------------------\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std =[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std =[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = val_transform  # same as val\n",
    "\n",
    "class CUBDataset(Dataset):\n",
    "    def __init__(self, df, transform):\n",
    "        self.paths = df[\"path\"].values\n",
    "        self.labels = df[\"class\"].values\n",
    "        self.transform = transform\n",
    "    def __len__(self): return len(self.paths)\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
    "        img = self.transform(img)\n",
    "        return img, self.labels[idx]\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(CUBDataset(train_df, train_transform),\n",
    "                          batch_size=batch_size, shuffle=True,\n",
    "                          num_workers=4, pin_memory=True, drop_last=True)\n",
    "val_loader   = DataLoader(CUBDataset(val_df , val_transform),\n",
    "                          batch_size=batch_size, shuffle=False,\n",
    "                          num_workers=4, pin_memory=True)\n",
    "test_loader  = DataLoader(CUBDataset(test_df, test_transform),\n",
    "                          batch_size=batch_size, shuffle=False,\n",
    "                          num_workers=4, pin_memory=True)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 7. DVML model (exact architecture)\n",
    "# --------------------------------------------------------------\n",
    "class DVML(nn.Module):\n",
    "    def __init__(self, embed_dim=512, T=20, dropout=0.3):\n",
    "        super().__init__()\n",
    "        goog = models.googlenet(weights=\"IMAGENET1K_V1\")\n",
    "        # keep everything up to avg-pool (1024-d)\n",
    "        self.backbone = nn.Sequential(*list(goog.children())[:-1])\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "\n",
    "        self.fc_I     = nn.Sequential(nn.Linear(1024, embed_dim), nn.Dropout(dropout))\n",
    "        self.fc_mu    = nn.Sequential(nn.Linear(1024, embed_dim), nn.Dropout(dropout))\n",
    "        self.fc_logvar= nn.Sequential(nn.Linear(1024, embed_dim), nn.Dropout(dropout))\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 1024)\n",
    "        )\n",
    "        self.T = T\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def encode(self, x):\n",
    "        f = self.pool(self.backbone(x)).view(x.size(0), -1)   # (B,1024)\n",
    "        z_I = self.fc_I(f)\n",
    "        mu  = self.fc_mu(f)\n",
    "        logvar = self.fc_logvar(f)\n",
    "        return z_I, mu, logvar, f\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x, phase=2):\n",
    "        z_I, mu, logvar, f = self.encode(x)\n",
    "        z_V = self.reparameterize(mu, logvar)               # (B,D)\n",
    "\n",
    "        # ---- T synthetic samples ----\n",
    "        z_V = z_V.unsqueeze(1).repeat(1, self.T, 1)          # (B,T,D)\n",
    "        z_I_rep = z_I.unsqueeze(1).repeat(1, self.T, 1)\n",
    "        z_hat = z_I_rep + z_V                               # (B,T,D)\n",
    "\n",
    "        z_hat_flat = z_hat.view(-1, self.embed_dim)         # (B*T,D)\n",
    "        f_hat_flat = self.decoder(z_hat_flat)               # (B*T,1024)\n",
    "\n",
    "        if phase == 1:                                      # stop decoder grads\n",
    "            f_hat_flat = f_hat_flat.detach()\n",
    "\n",
    "        return z_I, f, f_hat_flat, z_hat_flat, mu, logvar, z_hat\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 8. Proxy-NCA with label-smoothing & L2 on proxies\n",
    "# --------------------------------------------------------------\n",
    "class ProxyNCA(nn.Module):\n",
    "    def __init__(self, embed_dim, n_classes, temp=0.1, label_smoothing=0.1, proxy_l2=1e-4):\n",
    "        super().__init__()\n",
    "        self.proxies = nn.Parameter(torch.randn(n_classes, embed_dim))\n",
    "        nn.init.xavier_uniform_(self.proxies)\n",
    "        self.temp = temp\n",
    "        self.ls   = label_smoothing\n",
    "        self.l2   = proxy_l2\n",
    "\n",
    "    def forward(self, emb, label):\n",
    "        emb = F.normalize(emb, dim=1)\n",
    "        prox = F.normalize(self.proxies, dim=1)\n",
    "\n",
    "        sim = F.linear(emb, prox) / self.temp                # (B,C)\n",
    "\n",
    "        # label-smoothing\n",
    "        target = F.one_hot(label, num_classes=prox.size(0)).float()\n",
    "        target = target * (1-self.ls) + (self.ls / prox.size(0))\n",
    "\n",
    "        log_prob = F.log_softmax(sim, dim=1)\n",
    "        loss = - (target * log_prob).sum(dim=1).mean()\n",
    "\n",
    "        # L2 regularisation on proxies\n",
    "        if self.l2 > 0:\n",
    "            loss = loss + self.l2 * (prox ** 2).sum()\n",
    "        return loss\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 9. Loss helpers\n",
    "# --------------------------------------------------------------\n",
    "def kl_divergence(mu, logvar):\n",
    "    return -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "def recon_loss(f, f_hat_flat, T):\n",
    "    f_rep = f.repeat(T, 1)\n",
    "    return F.mse_loss(f_hat_flat, f_rep)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 10. Model / optimiser\n",
    "# --------------------------------------------------------------\n",
    "embed_dim = 512\n",
    "T = 20\n",
    "num_classes = len(train_classes)          # 100 known\n",
    "\n",
    "model       = DVML(embed_dim=embed_dim, T=T, dropout=0.3).to(device)\n",
    "proxy_I     = ProxyNCA(embed_dim, num_classes, temp=0.1,\n",
    "                       label_smoothing=0.1, proxy_l2=1e-4).to(device)\n",
    "proxy_hat   = ProxyNCA(embed_dim, num_classes, temp=0.1,\n",
    "                       label_smoothing=0.1, proxy_l2=1e-4).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(model.parameters()) + list(proxy_I.parameters()) + list(proxy_hat.parameters()),\n",
    "    lr=1e-4, weight_decay=1e-5\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 11. KL annealing schedule\n",
    "# --------------------------------------------------------------\n",
    "total_epochs = 120\n",
    "warmup_epochs = 30\n",
    "kl_start = 0.0\n",
    "kl_end   = 1.0\n",
    "\n",
    "def get_kl_weight(epoch):\n",
    "    if epoch <= warmup_epochs:\n",
    "        return kl_start + (kl_end - kl_start) * (epoch / warmup_epochs)\n",
    "    return kl_end\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 13. Feature extraction helper\n",
    "# --------------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def extract_feats(loader):\n",
    "    model.eval()\n",
    "    embs, lbls = [], []\n",
    "    for imgs, ls in loader:\n",
    "        imgs = imgs.to(device)\n",
    "        z_I, *_ = model(imgs, phase=2)\n",
    "        embs.append(z_I.cpu())\n",
    "        lbls.append(ls)\n",
    "    E = torch.cat(embs)\n",
    "    L = torch.cat(lbls)\n",
    "    E = F.normalize(E, dim=1).numpy()\n",
    "    L = L.numpy()\n",
    "    return E, L\n",
    "    \n",
    "# --------------------------------------------------------------\n",
    "# 14. Evaluation metrics\n",
    "# --------------------------------------------------------------\n",
    "def recall_at_k(feats, labels, k):\n",
    "    sim = feats @ feats.T\n",
    "    np.fill_diagonal(sim, -np.inf)\n",
    "    idx = np.argpartition(-sim, k, axis=1)[:, :k]\n",
    "    correct = 0\n",
    "    for i, neigh in enumerate(idx):\n",
    "        if labels[i] in labels[neigh]:\n",
    "            correct += 1\n",
    "    return correct / len(labels)\n",
    "\n",
    "def clustering_metrics(feats, labels, n_clusters):\n",
    "    km = KMeans(n_clusters=n_clusters, n_init=10, random_state=0)\n",
    "    pred = km.fit_predict(feats)\n",
    "    nmi = normalized_mutual_info_score(labels, pred)\n",
    "    f1  = f1_score(labels, pred, average='macro')\n",
    "    return nmi, f1\n",
    "\n",
    "def pairwise_prec_recall(feats, labels):\n",
    "    D = pairwise_distances(feats, metric='euclidean')\n",
    "    y_true = (labels[:, None] == labels[None, :]).astype(int)\n",
    "    thr = np.median(D)\n",
    "    y_pred = (D <= thr).astype(int)\n",
    "    tp = (y_pred * y_true).sum()\n",
    "    fp = (y_pred * (1-y_true)).sum()\n",
    "    fn = ((1-y_pred) * y_true).sum()\n",
    "    prec = tp / (tp + fp + 1e-12)\n",
    "    rec  = tp / (tp + fn + 1e-12)\n",
    "    return prec, rec\n",
    "# --------------------------------------------------------------\n",
    "# 12. Training loop (two phases + early-stop)\n",
    "# --------------------------------------------------------------\n",
    "phase1_epochs = 60\n",
    "phase2_epochs = total_epochs - phase1_epochs\n",
    "\n",
    "best_val_nmi = 0.0\n",
    "patience = 20\n",
    "wait = 0\n",
    "\n",
    "# loss weights (paper)\n",
    "λ1_p1, λ2_p1, λ3_p1, λ4_p1 = 1.0, 1.0, 0.1, 1.0\n",
    "λ1_p2, λ2_p2, λ3_p2, λ4_p2 = 0.8, 1.0, 0.2, 0.8\n",
    "\n",
    "for epoch in range(1, total_epochs + 1):\n",
    "    phase = 1 if epoch <= phase1_epochs else 2\n",
    "    λ_kl   = λ1_p1 if phase == 1 else λ1_p2\n",
    "    λ_recon= λ2_p1 if phase == 1 else λ2_p2\n",
    "    λ_hat  = λ3_p1 if phase == 1 else λ3_p2\n",
    "    λ_I    = λ4_p1 if phase == 1 else λ4_p2\n",
    "\n",
    "    model.train()\n",
    "    proxy_I.train()\n",
    "    proxy_hat.train()\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    for imgs, lbls in train_loader:\n",
    "        imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "\n",
    "        z_I, f, f_hat_flat, z_hat_flat, mu, logvar, _ = model(imgs, phase=phase)\n",
    "\n",
    "        L_kl   = kl_divergence(mu, logvar)\n",
    "        L_rec  = recon_loss(f, f_hat_flat, T)\n",
    "        L_I    = proxy_I(z_I, lbls)\n",
    "        lbl_hat= lbls.unsqueeze(1).repeat(1,T).view(-1)\n",
    "        L_hat  = proxy_hat(z_hat_flat, lbl_hat)\n",
    "\n",
    "        kl_w = get_kl_weight(epoch)\n",
    "        loss = (kl_w * λ_kl   * L_kl +\n",
    "                λ_recon * L_rec +\n",
    "                λ_I     * L_I +\n",
    "                λ_hat   * L_hat)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # ------------------- validation -------------------\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_emb, val_lbl = [], []\n",
    "        for imgs, lbls in val_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            z_I, *_ = model(imgs, phase=2)\n",
    "            val_emb.append(z_I.cpu())\n",
    "            val_lbl.append(lbls)\n",
    "        val_emb = torch.cat(val_emb).numpy()\n",
    "        val_lbl = torch.cat(val_lbl).numpy()\n",
    "        val_emb = val_emb / np.linalg.norm(val_emb, axis=1, keepdims=True)\n",
    "\n",
    "        nmi_val, _ = clustering_metrics(val_emb, val_lbl, n_clusters=num_classes)\n",
    "        print(f\"Epoch {epoch:03d} | Phase {phase} | Loss {epoch_loss/len(train_loader):.4f} | Val NMI {nmi_val:.4f}\")\n",
    "\n",
    "        # early-stop\n",
    "        if nmi_val > best_val_nmi:\n",
    "            best_val_nmi = nmi_val\n",
    "            torch.save(model.state_dict(), \"dvml_best.pth\")\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "\n",
    "# load best model\n",
    "model.load_state_dict(torch.load(\"dvml_best.pth\"))\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 15. Evaluate on TRAIN (known) and TEST (unseen)\n",
    "# --------------------------------------------------------------\n",
    "print(\"\\n=== TRAIN (known) SET ===\")\n",
    "train_E, train_L = extract_feats(train_loader)\n",
    "for k in [1,2,4,8]:\n",
    "    print(f\"Recall@{k}: {recall_at_k(train_E, train_L, k):.4f}\")\n",
    "nmi_tr, f1_tr = clustering_metrics(train_E, train_L, n_clusters=num_classes)\n",
    "print(f\"NMI : {nmi_tr:.4f}\")\n",
    "print(f\"F1  : {f1_tr:.4f}\")\n",
    "p_tr, r_tr = pairwise_prec_recall(train_E, train_L)\n",
    "print(f\"Prec: {p_tr:.4f}  Rec: {r_tr:.4f}\")\n",
    "\n",
    "print(\"\\n=== TEST (unseen) SET ===\")\n",
    "test_E, test_L = extract_feats(test_loader)\n",
    "for k in [1,2,4,8]:\n",
    "    print(f\"Recall@{k}: {recall_at_k(test_E, test_L, k):.4f}\")\n",
    "nmi_te, f1_te = clustering_metrics(test_E, test_L, n_clusters=len(test_classes))\n",
    "print(f\"NMI : {nmi_te:.4f}\")\n",
    "print(f\"F1  : {f1_te:.4f}\")\n",
    "p_te, r_te = pairwise_prec_recall(test_E, test_L)\n",
    "print(f\"Prec: {p_te:.4f}  Rec: {r_te:.4f}\")\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 16. PCA visualisation (test set)\n",
    "# --------------------------------------------------------------\n",
    "def plot_pca(E, L, title, n_samples=3000):\n",
    "    if len(E) > n_samples:\n",
    "        idx = np.random.choice(len(E), n_samples, replace=False)\n",
    "        E, L = E[idx], L[idx]\n",
    "    X = PCA(n_components=2).fit_transform(E)\n",
    "    plt.figure(figsize=(9,7))\n",
    "    s = plt.scatter(X[:,0], X[:,1], c=L, cmap='tab20', s=12, alpha=0.7)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\n",
    "    plt.colorbar(s)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_pca(test_E, test_L, \"PCA of z_I – 100 unseen classes\")\n",
    "\n",
    "\n",
    "def plot_tsne(E, L, title=\"t-SNE\", n_samples=3000, perplexity=30, random_state=42):\n",
    "    \"\"\"\n",
    "    Visualise embeddings E with labels L using t-SNE.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    E : np.ndarray, shape (N, D)\n",
    "        Embedding matrix.\n",
    "    L : np.ndarray, shape (N,)\n",
    "        Integer class labels.\n",
    "    title : str\n",
    "        Plot title.\n",
    "    n_samples : int\n",
    "        If N > n_samples, randomly subsample.\n",
    "    perplexity : float\n",
    "        t-SNE perplexity (typical values 5-50).\n",
    "    random_state : int\n",
    "        For reproducibility.\n",
    "    \"\"\"\n",
    "    if len(E) > n_samples:\n",
    "        idx = np.random.choice(len(E), n_samples, replace=False)\n",
    "        E, L = E[idx], L[idx]\n",
    "\n",
    "    # t-SNE (Barnes-Hut, O(N log N))\n",
    "    tsne = TSNE(\n",
    "        n_components=2,\n",
    "        perplexity=perplexity,\n",
    "        n_iter=1000,\n",
    "        random_state=random_state,\n",
    "        method='barnes_hut',   # fast approximate version\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    X_2d = tsne.fit_transform(E)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(\n",
    "        X_2d[:, 0], X_2d[:, 1],\n",
    "        c=L, cmap='tab20', s=15, alpha=0.7\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"t-SNE 1\")\n",
    "    plt.ylabel(\"t-SNE 2\")\n",
    "    plt.colorbar(scatter, label='Class')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Example usage (exactly like your PCA call)\n",
    "# ------------------------------------------------------------------\n",
    "plot_tsne(\n",
    "    test_E, test_L,\n",
    "    title=\"t-SNE of AutoEncoder + Proxy-NCA++ (100 Unseen Classes)\",\n",
    "    n_samples=3000,\n",
    "    perplexity=40          # tweak 20-50 for 100 classes\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
